import math
import time

import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import torch.optim as optim
from tqdm import tqdm

from model.attention import Attention
from model.decoder import Decoder
from model.encoder import Encoder
from model.hyperparams import *
from model.seq2seq import Seq2Seq
from tools.data_load import get_dataloaders_and_data, DOC, DEVICE
from tools.evaluate import evaluate
from tools.utils import safe_mkdir, refresh_cuda_memory

print("DEVICE: ", DEVICE)


def epoch_time(start_time, end_time):
    elapsed_time = end_time - start_time
    elapsed_mins = int(elapsed_time / 60)
    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))
    return elapsed_mins, elapsed_secs


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def init_weights(m):
    for name, param in m.named_parameters():
        if 'weight' in name:
            nn.init.normal_(param.data, mean=0, std=0.01)
        else:
            nn.init.constant_(param.data, 0)


def train(model, iterator, optimizer, criterion, clip, epoch):
    model.train()
    print("Now at epoch: ", epoch)

    epoch_loss = 0
#    refresh_cuda_memory()
    for batch in tqdm(iterator):
        src, src_len = batch.src
        trg, trg_len = batch.trg

        optimizer.zero_grad()

        output = model(src, src_len, trg)

        # trg = [trg len, batch size]
        # output = [trg len, batch size, output dim]

        output_dim = output.shape[-1]

        output = output[1:].view(-1, output_dim)
        trg = trg[1:].view(-1)

        # trg = [(trg len - 1) * batch size]
        # output = [(trg len - 1) * batch size, output dim]

        loss = criterion(output, trg)

        loss.backward()
        
        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)

        optimizer.step()

        epoch_loss += loss.item()
       
    return epoch_loss / len(iterator)


if __name__ == "__main__":
    safe_mkdir("plots")
    safe_mkdir("ckpts")
    train_iterator, valid_iterator, test_iterator, _, _, _ = get_dataloaders_and_data()
    SRC_PAD_IDX = DOC.vocab.stoi[DOC.pad_token]
    TRG_PAD_IDX = DOC.vocab.stoi[DOC.pad_token]
    INPUT_DIM = len(DOC.vocab)
    OUTPUT_DIM = len(DOC.vocab)

    attn = Attention(ENC_HID_DIM, DEC_HID_DIM)
    enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)
    dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)

    device = DEVICE
    CRITERION = nn.CrossEntropyLoss(ignore_index=TRG_PAD_IDX)

    model = Seq2Seq(enc, dec, SRC_PAD_IDX, device).to(device)
    print(f'The model has {count_parameters(model):,} trainable parameters')

    model.apply(init_weights)
    optimizer = optim.Adam(model.parameters())

    criterion = CRITERION

    best_valid_loss = float('inf')
    print("Starting to train model...")
    train_losses = []
    valid_losses = []
    epochs = []
    for epoch in range(N_EPOCHS):

        start_time = time.time()

        train_loss = train(model, train_iterator, optimizer, criterion, CLIP, epoch)
        valid_loss = evaluate(model, valid_iterator, criterion)
        train_losses.append(train_loss)
        valid_losses.append(valid_loss)
        epochs.append(epoch)
        end_time = time.time()

        epoch_mins, epoch_secs = epoch_time(start_time, end_time)

        if valid_loss < best_valid_loss:
            best_valid_loss = valid_loss
            torch.save(model.state_dict(), 'ckpts/ckpt_%d.pt' % epoch)

        long_string = f'Epoch: {epoch + 1:02} | Time: {epoch_mins}m {epoch_secs}s\n' +\
                      f'\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}\n' + \
                      f'\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}\n'
        print(long_string)
        plt.plot(epochs, train_losses, 'g', label='Training loss')
        plt.plot(epochs, valid_losses, 'b', label='Validation loss')
        plt.title('Training and Validation loss until epoch: %d' % epoch)
        plt.xlabel('Epochs')
        plt.ylabel('Loss')
        plt.legend()
        plt.savefig('plots/losses.png')
        plt.close()

