import matplotlib.pyplot as plt
import matplotlib.ticker as ticker
import torch
#from torchtext.data.metrics import bleu_score
import os

from model.attention import Attention
from model.decoder import Decoder
from model.encoder import Encoder
from model.hyperparams import *
from model.seq2seq import Seq2Seq
from tools.data_load import DOC, DEVICE, get_dataloaders_and_data, deal_with_file
from tools.utils import get_most_recent_ckpt

# DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
device = DEVICE


def translate_sentence(sentence, src_field, trg_field, model, device, max_len=50):
    model.eval()

    if isinstance(sentence, str):
        nlp = spacy.load('de')
        tokens = [token.text.lower() for token in nlp(sentence)]
    else:
        tokens = [token.lower() for token in sentence]

    tokens = [src_field.init_token] + tokens + [src_field.eos_token]

    src_indexes = [src_field.vocab.stoi[token] for token in tokens]

    src_tensor = torch.LongTensor(src_indexes).unsqueeze(1).to(device)

    src_len = torch.LongTensor([len(src_indexes)]).to(device)

    with torch.no_grad():
        encoder_outputs, hidden = model.encoder(src_tensor, src_len)

    mask = model.create_mask(src_tensor)

    trg_indexes = [trg_field.vocab.stoi[trg_field.init_token]]

    attentions = torch.zeros(max_len, 1, len(src_indexes)).to(device)

    for i in range(max_len):

        trg_tensor = torch.LongTensor([trg_indexes[-1]]).to(device)

        with torch.no_grad():
            output, hidden, attention = model.decoder(trg_tensor, hidden, encoder_outputs, mask)

        attentions[i] = attention

        pred_token = output.argmax(1).item()

        trg_indexes.append(pred_token)

        if pred_token == trg_field.vocab.stoi[trg_field.eos_token]:
            break

    trg_tokens = [trg_field.vocab.itos[i] for i in trg_indexes]

    return trg_tokens[1:], attentions[:len(trg_tokens) - 1], hidden


def display_attention(sentence, translation, attention):
    fig = plt.figure(figsize=(10, 10))
    ax = fig.add_subplot(111)

    attention = attention.squeeze(1).cpu().detach().numpy()

    cax = ax.matshow(attention, cmap='bone')

    ax.tick_params(labelsize=15)
    ax.set_xticklabels([''] + ['<sos>'] + [t.lower() for t in sentence] + ['<eos>'],
                       rotation=45)
    ax.set_yticklabels([''] + translation)

    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))
    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))

    plt.show()
    plt.close()


def calculate_bleu(data, src_field, trg_field, model, device, max_len=50):
    trgs = []
    pred_trgs = []

    for datum in data:
        src = vars(datum)['src']
        trg = vars(datum)['trg']

        pred_trg, _ = translate_sentence(src, src_field, trg_field, model, device, max_len)

        # cut off <eos> token
        pred_trg = pred_trg[:-1]

        pred_trgs.append(pred_trg)
        trgs.append([trg])

    return bleu_score(pred_trgs, trgs)

def init_model_from_ckpt():
    _, _, _, train_data, valid_data, test_data = get_dataloaders_and_data()
    SRC_PAD_IDX = DOC.vocab.stoi[DOC.pad_token]
    TRG_PAD_IDX = DOC.vocab.stoi[DOC.pad_token]
    INPUT_DIM = len(DOC.vocab)
    OUTPUT_DIM = len(DOC.vocab)

    attn = Attention(ENC_HID_DIM, DEC_HID_DIM)
    enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)
    dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)

    model = Seq2Seq(enc, dec, SRC_PAD_IDX, device).to(device)
    most_recent_ckpt = get_most_recent_ckpt('ckpts')
    model.load_state_dict(torch.load(most_recent_ckpt))
    return model, train_data, valid_data, test_data


def perform_inference_on_default_data():
    model, train_data, valid_data, test_data = init_model_from_ckpt()
    example_idx = 0
    src = vars(train_data.examples[example_idx])['src']
    trg = vars(train_data.examples[example_idx])['trg']

    print(f'src = {src}')
    print(f'trg = {trg}')
    translation, attention, _ = translate_sentence(src, DOC, DOC, model, device)

    print(f'predicted trg = {translation}')
    display_attention(src, translation, attention)

    example_idx = 0

    src = vars(valid_data.examples[example_idx])['src']
    trg = vars(valid_data.examples[example_idx])['trg']

    print(f'src = {src}')
    print(f'trg = {trg}')

    example_idx = 0

    src = vars(test_data.examples[example_idx])['src']
    trg = vars(test_data.examples[example_idx])['trg']

    print(f'src = {src}')
    print(f'trg = {trg}')
    translation, attention, _ = translate_sentence(src, DOC, DOC, model, device)
    print(f'predicted trg = {translation}')



def infer_from_filepath(filepath, model):
    example = deal_with_file(filepath)
    src = vars(example)['src']
    trg = vars(example)['trg']

    # print(f'src = {src}')
    # print(f'trg = {trg}')
    translation, attention, hidden = translate_sentence(src, DOC, DOC, model, device)
    return translation, hidden



if __name__ == "__main__":
     perform_inference_on_default_data()
     #model, _, _, _ = init_model_from_ckpt()
     #translation, hidden = infer_from_filepath(os.path.join("mount","final_400k", "data","9999.ps1"), model)
     #print(translation)
     #print(hidden)
 #   display_attention(src, translation, attention)

#    bleu_score = calculate_bleu(test_data, DOC, DOC, model, device)

#    print(f'BLEU score = {bleu_score * 100:.2f}')
