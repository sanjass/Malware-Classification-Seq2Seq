import multiprocessing as mp
import os
import pickle

import torch
from torchtext.data import Field, BucketIterator, Example, Dataset
from tqdm import tqdm

from tools.utils import read_file, safe_mkdir
import random

BATCH_SIZE = 2
DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
#DEVICE = 'cpu'  # force CPU usage if cuda breaks
device = DEVICE
torch.manual_seed(123)
random.seed(123)


def tokenize_char(text):
    """
    Tokenizes to sequences of 3 chars
    """
    res = " ".join(text.split())
    chunks, chunk_size = len(text) // 3, 3
    res = [res[i:i + chunk_size] for i in range(0, chunks, chunk_size)]
    #res = list(text)[:10000]
    res = res[:1000]
    return res


DOC = Field(tokenize=tokenize_char,
            init_token='<sos>',
            eos_token='<eos>',
            lower=True,
            include_lengths=True)


def deal_with_file(full_path):
    text = read_file(full_path)
    datapoint = Example.fromlist((text, text), fields=[("src", DOC), ("trg", DOC)])
    return datapoint


def create_dataset(data_path):
    if os.path.exists('data/dataset.pth'):
        data_examples = pickle.load(open('data/dataset.pth', 'rb'))

    else:
        pbar = tqdm(total=len(list(os.listdir(data_path))))

        data_examples = []
        pool = mp.Pool(mp.cpu_count())
        data_examples = []
        missed = 0

        def update(*args):
            args = args[0]
            pbar.update()
            if args is not None:
                data_examples.append(args)
            else:
                missed += 1

        for filename in os.listdir(data_path):
            pool.apply_async(deal_with_file, (os.path.join(data_path, filename),), callback=update)
        pool.close()
        pool.join()
        print("Missed files: ", missed)

        pickle.dump(data_examples, open('data/dataset.pth', 'wb'))
    dataset = Dataset(data_examples, fields={"src": DOC, "trg": DOC})
    return dataset


def get_dataloaders_and_data(dataset=None):
    if dataset is None:
        print("Loading saved data...Here")
        data_examples = pickle.load(open('data/dataset.pth', 'rb'))
        dataset = Dataset(data_examples[:10000], fields={"src": DOC, "trg": DOC})
        print("Finished loading saved data")
    train_data, valid_data, test_data = dataset.split(split_ratio=[0.7, 0.1, 0.2])
    DOC.build_vocab(train_data, min_freq=500)
    print("Vocab size: ", len(DOC.vocab))
    train_iterator, valid_iterator, test_iterator = BucketIterator.splits(
        (train_data, valid_data, test_data),
        batch_size=BATCH_SIZE,
        sort_within_batch=True,
        sort_key=lambda x: len(x.src),
        device=device)

    return train_iterator, valid_iterator, test_iterator, train_data, valid_data, test_data


if __name__ == "__main__":
    safe_mkdir("data")
    data_path = "mount/final_400k/data"
    dataset = create_dataset(data_path)
    get_dataloaders_and_data(dataset)
