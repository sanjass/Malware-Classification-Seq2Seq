import math

import torch
import torch.nn as nn

from model.attention import Attention
from model.decoder import Decoder
from model.encoder import Encoder
from model.hyperparams import *
from model.seq2seq import Seq2Seq
from tools.data_load import DOC, DEVICE, get_dataloaders_and_data
from tools.utils import get_most_recent_ckpt


device = DEVICE


def evaluate(model, iterator, criterion):
    model.eval()

    epoch_loss = 0

    with torch.no_grad():
        for i, batch in enumerate(iterator):
            src, src_len = batch.src
            trg, trg_len = batch.trg

            output = model(src, src_len, trg, 0)  # turn off teacher forcing

            # trg = [trg len, batch size]
            # output = [trg len, batch size, output dim]

            output_dim = output.shape[-1]

            output = output[1:].view(-1, output_dim)
            trg = trg[1:].view(-1)

            # trg = [(trg len - 1) * batch size]
            # output = [(trg len - 1) * batch size, output dim]

            loss = criterion(output, trg)

            epoch_loss += loss.item()

        return epoch_loss / len(iterator)


if __name__ == "__main__":
    _, _, test_iterator, _, _, _ = get_dataloaders_and_data()
    SRC_PAD_IDX = DOC.vocab.stoi[DOC.pad_token]
    TRG_PAD_IDX = DOC.vocab.stoi[DOC.pad_token]
    INPUT_DIM = len(DOC.vocab)
    OUTPUT_DIM = len(DOC.vocab)
    CRITERION = nn.CrossEntropyLoss(ignore_index=TRG_PAD_IDX)

    attn = Attention(ENC_HID_DIM, DEC_HID_DIM)
    enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)
    dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)

    model = Seq2Seq(enc, dec, SRC_PAD_IDX, device).to(device)
    most_recent_ckpt = get_most_recent_ckpt('ckpts')
    model.load_state_dict(torch.load(most_recent_ckpt)) # if you want to use other ckpt 
                                                        # replace most_recent_ckpt with the path

    test_loss = evaluate(model, test_iterator, CRITERION)

    print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')
